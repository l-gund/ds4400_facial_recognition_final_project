{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AiO5Ke5UHfM3",
        "outputId": "4143dec1-53be-45e3-b2a3-5c96f9eb37a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'4.5.5'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "import sys  \n",
        "cv2.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G5B0VKfsObUd",
        "outputId": "7862a0da-58af-4127-8d32-f8de9a2a5462"
      },
      "outputs": [
        {
          "ename": "error",
          "evalue": "OpenCV(4.5.5) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - imshow() missing required argument 'mat' (pos 2)\n>  - imshow() missing required argument 'mat' (pos 2)\n>  - imshow() missing required argument 'mat' (pos 2)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Lilly\\Downloads\\DS4400_Facial_recognition_project.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lilly/Downloads/DS4400_Facial_recognition_project.ipynb#ch0000001?line=6'>7</a>\u001b[0m image_2 \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(image, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lilly/Downloads/DS4400_Facial_recognition_project.ipynb#ch0000001?line=7'>8</a>\u001b[0m final_frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mhconcat((image, image_2))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Lilly/Downloads/DS4400_Facial_recognition_project.ipynb#ch0000001?line=8'>9</a>\u001b[0m cv2\u001b[39m.\u001b[39;49mimshow(final_frame)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lilly/Downloads/DS4400_Facial_recognition_project.ipynb#ch0000001?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - imshow() missing required argument 'mat' (pos 2)\n>  - imshow() missing required argument 'mat' (pos 2)\n>  - imshow() missing required argument 'mat' (pos 2)\n"
          ]
        }
      ],
      "source": [
        "urls = [\"https://t4.ftcdn.net/jpg/03/83/25/83/360_F_383258331_D8imaEMl8Q3lf7EKU2Pi78Cn0R7KkW9o.jpg\",\n",
        "\"https://d2cbg94ubxgsnp.cloudfront.net/Pictures/2000xAny/2/7/8/513278_shutterstock_1544594165_719610.jpg\",\n",
        "\"https://hips.hearstapps.com/countryliving.cdnds.net/17/47/1511194376-cavachon-puppy-christmas.jpg\"]\n",
        "\n",
        "for url in urls:\n",
        "  image = io.imread(url)\n",
        "  image_2 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  final_frame = cv2.hconcat((image, image_2))\n",
        "  cv2.imshow(final_frame)\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FXvyI5NMQa5V",
        "outputId": "7e816c9b-e49b-4f9d-fd1b-d0a16476cdb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 17 faces!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get user supplied values\n",
        "imagePath = '02-still-for-america-room-loop-superJumbo.jpg'\n",
        "cascPath = 'haarcascade_frontalface_default.xml'\n",
        "\n",
        "# Create the haar cascade\n",
        "faceCascade = cv2.CascadeClassifier(cascPath)\n",
        "\n",
        "# Read the image\n",
        "image = cv2.imread(imagePath)\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Detect faces in the image\n",
        "faces = faceCascade.detectMultiScale(\n",
        "    gray,\n",
        "    scaleFactor=1.2,\n",
        "    minNeighbors=5,\n",
        "    minSize=(30, 30)\n",
        "    #flags = cv2.CV_HAAR_SCALE_IMAGE\n",
        ")\n",
        "\n",
        "print(\"Found {0} faces!\".format(len(faces)))\n",
        "\n",
        "# Draw a rectangle around the faces\n",
        "for (x, y, w, h) in faces:\n",
        "    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "cv2.imshow('Found face',image)\n",
        "cv2.waitKey(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '26_1_2_20170116173312689.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '54_0_0_20170120223352027.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '24_0_4_20170113151020448.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '26_0_3_20170119180456692.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '5_0_0_20170110213322127.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '1_1_0_20170109190818115.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '26_1_0_20170119192623081.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '24_1_3_20170104214534021.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '50_0_3_20170119195739498.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '30_0_3_20170117150038298.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '22_1_0_20170117141131710.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '34_1_1_20170113001015235.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '50_0_0_20170120222405224.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '90_0_2_20170111210740854.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '32_1_0_20170104165842857.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '14_1_0_20170109203405475.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '28_0_0_20170114030950715.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '26_1_3_20170119144411037.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '43_0_0_20170117155042830.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '26_0_4_20170117153017629.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '26_1_1_20170116182045931.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '1_1_0_20170109192755957.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '42_1_4_20170117171425948.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '25_1_0_20170116223848612.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '28_1_2_20170116161415319.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '1_1_0_20161219205022813.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '55_0_0_20170117190804738.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '45_0_1_20170117181136802.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '26_1_3_20170119193125362.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '28_0_2_20170104021218340.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '30_0_0_20170105170132300.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '39_1_1_20170116224149881.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '2_0_2_20161219194834811.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '1_1_2_20161219222758263.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '13_1_0_20170109205049477.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '10_0_0_20170110225421531.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '23_0_0_20170119145312190.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '56_0_1_20170113174030713.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '25_1_0_20170116212058157.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '75_0_2_20170110131323894.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '24_1_1_20170116022047558.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '35_0_1_20170113134314369.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '28_0_3_20170119194426537.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '30_1_2_20170116180519798.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '29_1_0_20170103183239274.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '35_0_4_20170117153010428.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '15_1_2_20170104012024121.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '62_0_1_20170111204710125.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '20_1_2_20170104020440101.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '67_0_0_20170120224529039.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '1_0_2_20161219212510086.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '23_0_0_20170117144550331.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '27_1_0_20170117120648293.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '36_1_0_20170117091704944.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '55_0_0_20170117190004928.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '27_0_1_20170117172452375.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '45_0_0_20170120221555460.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '39_0_0_20170105165203483.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '38_1_1_20170104234648395.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '24_1_2_20170116171543349.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '55_0_0_20170117190131875.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '40_1_1_20170113001356402.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '39_1_0_20170103182509962.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '65_0_0_20170113190804729.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '55_0_0_20170120222352831.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '42_1_1_20170113001802155.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '65_1_0_20170120225024135.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '28_1_3_20170119204506255.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '50_0_3_20170117190730009.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '45_0_0_20170117203724896.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '40_0_4_20170104202246139.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '20_0_0_20170116205352042.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '50_0_3_20170119204709807.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '28_1_0_20170116192508046.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '34_0_0_20170116214408892.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '41_1_1_20170117021604893.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '26_0_1_20170116203607574.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '36_1_0_20170116003333930.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '2_1_4_20161221192715844.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '3_1_2_20161219204943868.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '61_1_3_20170112231727406.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '26_1_0_20170117174144110.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '45_0_4_20170104184343710.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '35_0_4_20170105162641195.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '27_0_3_20170104214555317.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '40_0_2_20170113190355448.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '37_0_0_20170117120612576.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '35_0_0_20170120220720314.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '75_0_0_20170111205905746.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '28_1_0_20170108225418569.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '28_1_0_20170116221705370.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '18_0_1_20170114033130286.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '36_0_0_20170117175609824.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '34_0_0_20170116192050166.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '60_1_1_20170120222320336.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '1_1_4_20170117192625477.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '25_1_3_20161220221512186.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '26_1_1_20170116233102426.jpg.chip.jpg'},\n",
              " {'gender': 'male',\n",
              "  'race': 'white',\n",
              "  'path': '1_0_3_20161220143114408.jpg.chip.jpg'},\n",
              " {'gender': 'female',\n",
              "  'race': 'black',\n",
              "  'path': '22_1_0_20170117171049498.jpg.chip.jpg'}]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os, random\n",
        "#normalization open cv\n",
        "random_face_group = []\n",
        "num_picks = 100\n",
        "for i in range(num_picks):\n",
        "    face_str = random.choice(os.listdir('UTKFace'))\n",
        "    face_str_split = face_str.split('_')\n",
        "    if(face_str_split[1]=='0'):\n",
        "        gender = 'male'\n",
        "    else:\n",
        "        gender = 'female'\n",
        "    \n",
        "    #set race\n",
        "    if face_str_split[1]=='0':\n",
        "        race='white'\n",
        "    elif face_str_split[1]=='1':\n",
        "        race = 'black'\n",
        "    elif face_str_split[1]=='2':\n",
        "        race='Asian'\n",
        "    elif face_str_split[1]=='3':\n",
        "        race='Indian'\n",
        "    else:\n",
        "        race='other'\n",
        "    face_dict = {'gender':gender,'race':race,'path':face_str}\n",
        "    random_face_group.append(face_dict)\n",
        "\n",
        "random_face_group"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DS4400 Facial recognition project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
